---
layout: page
title: About
---

## YUVRAJ SINGH

**India** | [singhyuvrajf1@gmail.com](mailto:singhyuvrajf1@gmail.com) | +918178693375  
[linkedin.com/in/yuvraj-singh-4988ba371](https://www.linkedin.com/in/yuvraj-singh-4988ba371/) | [github.com/Yuvrajxms09](https://github.com/Yuvrajxms09)

**Interests:** tennis, formula 1, dancing

---

## EXPERIENCE

### Machine Learning Intern
**Dria.co**  
Nov 2025 – Present

• Integrated Model Context Protocol (MCP) to expose DNET for AI agent interaction  
• Implemented schema-constrained generation with Outlines and xGrammar  
• Added TensorRT support for major LLMs (Qwen, DeepSeek, GPT-OSS) across the benchmarking system

### Machine Learning Intern
**Eachlabs.ai**  
May 2025 – Sep 2025

• Led optimization of models (Wan, Qwen, Flux), achieving >3x reduction in inference latency  
• Implemented distributed training and inference, improving scalability and reducing GPU costs  
• Implemented MLOps best practices with finetuning/LoRA to enhance enterprise workflows

---

## NOTABLE OPEN SOURCE CONTRIBUTIONS

### Summary Bot (Nextcloud)
• Integrated SQLAlchemyJobStore with SQLite to retain scheduled tasks across restarts  
• Improved reliability by enabling APScheduler to handle persistent task data seamlessly

### Translate2 (Nextcloud)
• Optimized performance by persistently loading models to eliminate reload delays  
• Added thread locks to handle concurrent translation requests

---

## PROJECTS

### BioBERT-Triage
• Fine-tuned BioBERT to classify medical complaints as Urgent or Non-Urgent  
• Optimized training to reduce loss by 81% (from 3.0 to 0.56)  
• Maintained balanced data handling to prevent bias and ensure real-world generalization

### MediMistral
• Fine-tuned Mistral 7B on clinical notes using QLoRA for efficient low-resource training  
• Applied LoRA adapters to key transformer layers for parameter efficient fine-tuning  
• Reduced training loss by 79% (from 5.6 to 1.1) via hyperparameter tuning

---
